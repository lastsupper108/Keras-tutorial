{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gan_manoj.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNxRzL2iGJ0yOZ8Or48gOCQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lastsupper108/Keras-tutorial/blob/master/gan_manoj.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKCzPRtvzMQ0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8717a3fd-b526-4984-fdc0-db67024be756"
      },
      "source": [
        "from scipy import signal\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "### 1D traiangle data generation ###\n",
        "DisInputLength= 128\n",
        "GenInputLenght =10\n",
        "batch_size = 12\n",
        "learning_rate = 5e-3 \n",
        "num_training_iterations= 200\n",
        "\n",
        "def get_Gen_inp(InputLenght,batch):\n",
        "\treturn np.random.rand(batch,InputLenght)\n",
        "\n",
        "def get_Dist_real(Lenght,batch):\n",
        "\tnormal_shift = (np.random.normal(0,10,batch)).astype(int)\n",
        "\twave = signal.triang(Lenght)\n",
        "\tx_input = np.array([np.roll(wave,x) for x in normal_shift])\n",
        "\treturn x_input\n",
        "\n",
        "def build_Gen(GenInputLenght,DisInputLength):\n",
        "  model = tf.keras.Sequential(\\\n",
        "  \t[tf.keras.layers.Dense(16,input_shape=[GenInputLenght])\\\n",
        "  \t,tf.keras.layers.Dense(32)\\\n",
        "  \t,tf.keras.layers.Dense(64)\\\n",
        "  \t,tf.keras.layers.Dense(DisInputLength)])\n",
        "  return model\n",
        "\n",
        "def build_Dis(DisInputLength):\n",
        "  model = tf.keras.Sequential(\\\n",
        "  \t[tf.keras.layers.Dense(64,input_shape=[DisInputLength])\\\n",
        "  \t,tf.keras.layers.Dense(32)\\\n",
        "  \t,tf.keras.layers.Dense(16)\\\n",
        "  \t,tf.keras.layers.Dense(1,activation='sigmoid')])\n",
        "  return model\n",
        "\n",
        "Gen = build_Gen(GenInputLenght,DisInputLength)\n",
        "Dis = build_Dis(DisInputLength)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "checkpoint_dir_gen = './'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir_gen, \"my_ckpt_gen\")\n",
        "checkpoint_dir_dis = './'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir_dis, \"my_ckpt_dis\")\n",
        "\n",
        "\n",
        "def compute_loss(labels, pred):\n",
        "\tloss = tf.keras.losses.binary_crossentropy(labels, pred)\n",
        "\treturn loss\n",
        "\n",
        "#@tf.function\n",
        "def train_step(gen_x, dis_real_x, dis_y,gen_y): \n",
        "  with tf.GradientTape(persistent=True) as tape:\n",
        "    gen_output = Gen(gen_x)\n",
        "    dis_real_out = Dis(dis_real_x)\n",
        "    dis_fake_out = Dis(gen_output)\n",
        "    dis_pred = tf.concat([dis_real_out,dis_fake_out],0)\n",
        "    #print(tf.squeeze(dis_pred).numpy(),dis_y)\n",
        "    loss_gen = compute_loss(gen_y,tf.squeeze(dis_fake_out))\n",
        "    loss_dis = compute_loss(dis_y,tf.squeeze(dis_pred))\n",
        "\n",
        "  #print(np.shape(loss_gen.numpy()),np.shape(loss_dis.numpy()))\n",
        "  #print(loss_dis.numpy())\n",
        "  grads_dis = tape.gradient(loss_dis, Dis.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads_dis, Dis.trainable_variables))\n",
        "\n",
        "  grads_gen = tape.gradient(loss_gen, Gen.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads_gen, Gen.trainable_variables))\n",
        "\n",
        "  return loss_dis+loss_gen\n",
        "\n",
        "\n",
        "#### training ########\n",
        "for iter in range(num_training_iterations):\n",
        "\tgen_x = get_Gen_inp(GenInputLenght,batch_size)\n",
        "\tdis_real_x = get_Dist_real(DisInputLength,batch_size)\n",
        "\n",
        "\t#other half of disc input is gen output\n",
        "\tdis_y = np.concatenate([np.ones(batch_size),np.zeros(batch_size)])\n",
        "\tgen_y = np.ones(batch_size)\n",
        "\n",
        "\tloss = train_step(gen_x,dis_real_x,dis_y,gen_y)\n",
        "\n",
        "\tprint(loss.numpy().mean())\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.2608254\n",
            "1.0285847\n",
            "0.9747412\n",
            "0.96134424\n",
            "0.9749553\n",
            "0.98520356\n",
            "1.0027666\n",
            "1.402098\n",
            "2.292549\n",
            "2.2029045\n",
            "1.5720105\n",
            "2.5676556\n",
            "3.7029803\n",
            "2.5768056\n",
            "4.7482142\n",
            "5.4938936\n",
            "2.819575\n",
            "1.470782\n",
            "2.382333\n",
            "2.9391947\n",
            "3.9091797\n",
            "4.6376677\n",
            "4.424519\n",
            "5.970211\n",
            "5.2252264\n",
            "6.673463\n",
            "8.452736\n",
            "4.1949973\n",
            "7.7838984\n",
            "7.353462\n",
            "7.6438475\n",
            "7.940082\n",
            "9.32593\n",
            "8.95924\n",
            "7.842359\n",
            "7.666865\n",
            "7.666696\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n",
            "7.66662\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}